下面是一套可直接落地到你们 AI 服装处理 SaaS 的**混合式（协同过滤为主）推荐系统**方案，覆盖：总体架构、算法选型、特征工程、相似度与冷启动、在线推理架构（<50ms）、A/B 测试与评估指标，并附\*\*完整 Python 代码（训练+推理）\*\*示例，代码结构按你给的数据标准字段而写，拿到你们的日志即可替换接入。

---

## 0. 总览（两阶段 Hybrid 推荐）

**目标**：把“用户最可能用的功能”排前、新手首屏 3 个核心功能、基于使用的相似功能推荐、套餐推荐、冷启动稳健、在线延迟 <50ms、支持 A/B。

**策略**：采用工业界常见的**两阶段**：

1. **召回（Candidate Generation）**：

   * **ItemCF**（主力，基于相似功能/相似用法）
   * **UserCF**（相似用户的常用功能）
   * **人群先验/内容召回**（基于行业/付费/频率 → 功能偏好先验）
   * **流行度/成功率**（全局与分行业）

   > 召回量通常 50\~200 个，20+ 功能体量更轻，可直接 20 全量评分。
2. **精排（Scoring）**：

   * 轻量打分融合：`Score = w_itemcf + w_usercf + w_content + w_pop`（可用逻辑回归/GBDT 学习权重；先用 rule-based 再演进）
   * **MMR 多样化**：避免前三都同类，如“抠图 & 换装 & 去水印”各一类
   * **探索（Bandit）**：冷启动或长尾功能以小流量探索（汤普森采样）

---

## 1. 推荐算法选型

* **主推：协同过滤（User-based + Item-based）**

  * 你们只有 20+ 个功能，离线即可把**用户-功能稀疏矩阵**跑透；ItemCF 稳定、可解释且实时开销极小（只查邻居）。
* **内容/人群先验**（补充 CF 的冷启动）：

  * 从**用户画像×功能属性**的统计先验估计 $P(item \mid profile)$（比如“电商-付费-高频”偏好“批量换装+抠图”）。
* **深度学习（可选后续）**

  * 若未来功能 >100、行为序列长，可加 **SASRec/GRU4Rec** 做序列召回或使用 **双塔**做表征学习。当前 20+ 功能阶段，成本＞收益，不作为首发方案。

---

## 2. 特征工程（最小可行+可演进）

* **用户侧**

  * *强特征*：行业、是否付费、近 7/30 天活跃频次、最近 1/3 个使用功能（序列）
  * *派生*：平均会话长度、成功率、是否 API/批量偏好
* **功能侧**

  * *静态属性*：任务类别（抠图/换装/去水印/修复/批处理/API）、是否付费专属、难度/时长估计
  * *统计属性*：全局/分行业的使用次数、成功率、留存贡献
* **交叉**

  * (行业 × 功能类别) 的历史转化先验；(付费 × 功能) 的成功率

> 行为强度建议加**时间衰减**：$w = \exp(-\Delta t / \tau)$（$\tau$ 7\~30 天），再按是否成功加权 $w \cdot (1+\gamma\cdot \text{success})$。

---

## 3. 相似度计算（核心）

* **Item 相似度（功能相似度）**：

  * 用户-功能加权矩阵 $R$（行=用户，列=功能）。令 $\hat{R}$ 为列向量 L2 归一后，

    $$
    S_{\text{item}} = \hat{R}^\top \hat{R}\quad(\text{余弦相似})
    $$
  * 补充分行业/成功率的 TF‑IDF/BM25 加权（可选）。
* **User 相似度（用户相似度）**：

  $$
  S_{\text{user}} = \tilde{R}\tilde{R}^\top\quad(\text{行 L2 归一后的余弦})
  $$
* **混合**：

  $$
  \text{score}(u, i)=
  \underbrace{R_{u,*}S_{\text{item}}}_{\text{ItemCF}}
  + \lambda \underbrace{S_{\text{user}}(u,*)R}_{\text{UserCF}}
  + \alpha \cdot \text{ContentPrior}(u,i)
  + \beta \cdot \text{Popularity}(i)
  $$

---

## 4. 冷启动策略

* **新用户**：

  1. **人群先验** $P(i\mid \text{industry},\text{paid},\text{freq})$ + **分行业流行度** + **成功率**
  2. **探索（Bandit）**：对先验 Top5 做少量探索（Beta(α,β) 汤普森采样），避免只推热门
  3. **新手引导 3 个功能**：先验 TopN + **MMR 多样化**（不同类别）
* **新功能**：

  * 用**功能属性相似**（类别/耗时/是否付费）+ 与同类旧功能共现的 **内容相似/上下游关系** 初始化邻居；前期用 Bandit 探索流量

---

## 5. 实时推荐架构（<50ms）

```
(批/流 ETL)          (离线训练/预计算)                  (在线服务 <50ms)
Kafka/Log  --->  Spark/Beam + Airflow  --->  1) R矩阵、S_item、S_user、先验
                                          2) 人群先验表、流行度、成功率
                                          3) Bandit 统计（α/β）
                                   ┌──────────────┬─────────────────────┐
Feature Store(离线)  --->  Online Store(如 Redis/KeyDB)  --->  Reco API(gRPC/HTTP)
                                   └──────────────┴─────────────────────┘
                                                      |
                                                  SDK/前端
```

* **在线路径**（单用户）：

  1. 取最近使用功能/画像（Redis 哈希，\~1–3ms）
  2. 查邻居列表（S\_item 的 TopK 已预计算入 Redis，\~1–3ms）
  3. 合并分数（内存向量运算，\~0.5–2ms）
  4. MMR 多样化 + 过滤权限（\~1–3ms）
  5. 返回 TopN（总 < 20ms；即使跨区也<50ms）
* **容灾**：Redis miss → 回退“人群先验 + 流行度”；日志全部写入 Kafka

---

## 6. A/B 测试设计

* **随机单元**：`user_id`（固定分桶，防串组）
* **实验维度**：

  * 控制组：简单“分行业流行度”排序
  * 实验组：Hybrid（ItemCF+UserCF+先验+流行度+MMR+Bandit）
* **样本量**：基于**两比例差**（CTR 或 功能点击/使用率），设 MDE=+5% 相对提升，α=0.05, power=0.8。
* **指标**：

  * 主要：**功能点击率/使用率（CTR/UTR）**、**首日/首周转化率（从试用到付费或从浏览到实际调用）**
  * 业务：**功能成功率**、**工单率**、**套餐升级率/ARPU**、**7/30天留存**
  * 护栏：页面加载时延、错误率、投诉率
* **统计**：两比例 Z 检验 + 置信区间；必要时用**序贯检验（SPRT）**或**组序贯**避免提前停留偏差
* **曝光控制**：首屏 3 个推荐用于“新手引导”点击位，计入曝光分母

---

## 7. 推荐效果评估（离线 + 在线）

* **离线切分**：每用户留出**最近一次**作为测试样本（Leave‑One‑Out），其余训练
* **Top‑K 指标**：HitRate\@K、Recall\@K、MAP\@K、NDCG\@K
* **业务指标对齐**：用“成功完成一次处理”当 label（而非仅点击），保持离在线一致
* **在线**：A/B 分析 CTR、转化、留存、升级率，至少跑满一个**业务周期**（如计费周期）

---

## 8. 完整 Python 代码（训练 + 推理）

> 说明
>
> * 适配你的数据：
>
>   * 行为日志：`logs[user_id, feature_id, created_at, success]`（`success`可选）
>   * 用户画像：`users[user_id, industry, paid, usage_freq]`
>   * 功能表：`features[feature_id, category, requires_paid]`
> * 依赖：`numpy`, `pandas`, `scipy`（仅稀疏矩阵），无需深度学习框架。
> * 包含：ItemCF/UserCF 相似度、内容先验、冷启动、Bandit、MMR、多指标评估、在线推理函数与“相似功能推荐”“新手 3 个功能”“套餐推荐（示例）”。

```python
# -*- coding: utf-8 -*-
import math
import random
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix, issparse

# -----------------------------
# 工具函数
# -----------------------------
def build_id_map(ids: List) -> Tuple[Dict, Dict]:
    uniq = pd.Index(pd.Series(ids).astype(str).unique())
    id2idx = {k: i for i, k in enumerate(uniq)}
    idx2id = {i: k for k, i in id2idx.items()}
    return id2idx, idx2id

def to_datetime_col(df, col):
    if col in df.columns and not np.issubdtype(df[col].dtype, np.datetime64):
        df[col] = pd.to_datetime(df[col])
    return df

def time_decay_weight(days, tau=14.0):
    return np.exp(-np.clip(days, 0, None) / float(tau))

def normalize_rows_sparse(mat: csr_matrix) -> csr_matrix:
    """Row-wise L2 normalize for CSR matrix."""
    if not issparse(mat):  # convert
        mat = csr_matrix(mat)
    row_sums = np.sqrt(mat.power(2).sum(axis=1)).A1
    row_sums[row_sums == 0] = 1.0
    inv = 1.0 / row_sums
    D = csr_matrix((inv, (np.arange(mat.shape[0]), np.arange(mat.shape[0]))), shape=(mat.shape[0], mat.shape[0]))
    return D.dot(mat)

def topk_per_row(mat: csr_matrix, k: int) -> csr_matrix:
    """Keep top-k per row of a CSR cosine-sim matrix."""
    mat = mat.tocsr().astype(np.float32)
    indptr, indices, data = mat.indptr, mat.indices, mat.data
    new_indptr = [0]
    new_indices = []
    new_data = []
    for i in range(mat.shape[0]):
        start, end = indptr[i], indptr[i+1]
        row_idx = indices[start:end]
        row_val = data[start:end]
        if end - start > k:
            topk_idx = np.argpartition(row_val, -k)[-k:]
            rr = topk_idx[np.argsort(-row_val[topk_idx])]
        else:
            rr = np.argsort(-row_val)
        new_indices.extend(row_idx[rr])
        new_data.extend(row_val[rr])
        new_indptr.append(len(new_indices))
    return csr_matrix((np.array(new_data), np.array(new_indices), np.array(new_indptr)), shape=mat.shape)

def mmr_diversify(candidates: List[str], scores: Dict[str, float], item2cat: Dict[str, str], k=3, lam=0.8):
    """Maximal Marginal Relevance by category to increase diversity."""
    selected = []
    cats = []
    for _ in range(min(k, len(candidates))):
        best, best_score = None, -1e9
        for x in candidates:
            if x in selected: 
                continue
            rel = scores.get(x, 0.0)
            div = 0.0
            for s, c in zip(selected, cats):
                div = max(div, 1.0 if item2cat.get(x) == c else 0.0)  # same category penalize
            score = lam * rel - (1 - lam) * div
            if score > best_score:
                best, best_score = x, score
        if best is None: 
            break
        selected.append(best)
        cats.append(item2cat.get(best, "NA"))
    return selected

# -----------------------------
# 配置
# -----------------------------
@dataclass
class RecoConfig:
    tau_days: float = 14.0              # 时间衰减窗口
    success_gamma: float = 0.2          # 成功加权
    topk_neighbor_item: int = 10
    topk_neighbor_user: int = 50
    weight_itemcf: float = 0.50
    weight_usercf: float = 0.30
    weight_content: float = 0.10
    weight_pop: float = 0.10
    mmr_lambda: float = 0.8
    bandit_alpha: float = 1.0
    bandit_beta: float = 1.0

# -----------------------------
# 主类：Hybrid CF Recommender
# -----------------------------
class HybridCFRecommender:
    def __init__(self, config: RecoConfig = RecoConfig()):
        self.cfg = config
        # 索引映射
        self.user2idx = {}
        self.idx2user = {}
        self.item2idx = {}
        self.idx2item = {}
        # 矩阵/先验
        self.R = None               # 用户-功能矩阵（加权）
        self.S_item = None          # 功能相似度
        self.S_user = None          # 用户相似度
        self.pop_vec = None         # 流行度/成功率
        self.content_prior = None   # P(item | profile) per user
        self.item_category = {}     # feature_id -> category
        # Bandit 先验（功能层面）
        self.bandit_alpha = None
        self.bandit_beta = None

    # ---------- 训练 ----------
    def fit(self, logs: pd.DataFrame, users: pd.DataFrame, features: pd.DataFrame):
        # 标准化字段类型
        logs = logs.copy()
        users = users.copy()
        features = features.copy()
        logs['user_id'] = logs['user_id'].astype(str)
        logs['feature_id'] = logs['feature_id'].astype(str)
        users['user_id'] = users['user_id'].astype(str)
        features['feature_id'] = features['feature_id'].astype(str)
        to_datetime_col(logs, 'created_at')
        if 'success' not in logs.columns:
            logs['success'] = 1  # 没有成功标记时当作隐式正反馈

        # 索引映射（包含所有功能）
        all_items = pd.Index(features['feature_id'].unique()).union(pd.Index(logs['feature_id'].unique()))
        self.user2idx, self.idx2user = build_id_map(logs['user_id'])
        self.item2idx, self.idx2item = build_id_map(all_items.tolist())

        # 功能类别字典
        if 'category' in features.columns:
            self.item_category = dict(zip(features['feature_id'], features['category']))
        else:
            self.item_category = {fid: 'NA' for fid in all_items}

        # 聚合权重（时间衰减 + 成功加权）
        now = logs['created_at'].max()
        days = (now - logs['created_at']).dt.days.astype(float)
        w_time = time_decay_weight(days, self.cfg.tau_days)
        succ = logs['success'].astype(int).values if 'success' in logs.columns else np.ones(len(logs))
        weights = w_time * (1.0 + self.cfg.success_gamma * succ)

        g = logs.groupby(['user_id', 'feature_id']).size().rename('cnt').reset_index()
        # 我们用加权而不是纯次数
        g_w = logs.copy()
        g_w['w'] = weights
        g_w = g_w.groupby(['user_id', 'feature_id'])['w'].sum().reset_index()

        # 构建 R 矩阵
        rows = g_w['user_id'].map(self.user2idx).values
        cols = g_w['feature_id'].map(self.item2idx).values
        data = g_w['w'].values
        n_users = len(self.user2idx)
        n_items = len(self.item2idx)
        self.R = csr_matrix((data, (rows, cols)), shape=(n_users, n_items))

        # ItemCF 相似度
        X = normalize_rows_sparse(self.R.T)             # item-by-user
        self.S_item = (X @ X.T).tocsr()
        self.S_item.setdiag(0)
        self.S_item = topk_per_row(self.S_item, k=min(self.cfg.topk_neighbor_item, n_items-1))

        # UserCF 相似度
        Y = normalize_rows_sparse(self.R)               # user-by-item
        self.S_user = (Y @ Y.T).tocsr()
        self.S_user.setdiag(0)
        self.S_user = topk_per_row(self.S_user, k=min(self.cfg.topk_neighbor_user, n_users-1))

        # 流行度/成功率
        pop = np.asarray(self.R.sum(axis=0)).ravel()
        if 'success' in logs.columns:
            succ_by_item = logs.groupby('feature_id')['success'].mean().reindex(all_items).fillna(0.0).values
            pop = pop * (0.5 + 0.5 * succ_by_item)   # 兼顾数量与成功率
        self.pop_vec = (pop - pop.min()) / (pop.ptp() + 1e-9)

        # 内容先验（人群 × 功能）
        #   例：P(item | industry) 与 P(item | paid) 的贝叶斯平滑，合成用户级 prior
        merged = logs[['user_id', 'feature_id']].merge(users, on='user_id', how='left') \
                                               .merge(features[['feature_id', 'category', 'requires_paid']], on='feature_id', how='left')
        # industry
        if 'industry' in users.columns:
            c_ind = merged.groupby(['industry', 'feature_id']).size().unstack(fill_value=0)
            c_ind = c_ind.reindex(columns=all_items, fill_value=0)
            # 拉普拉斯平滑
            c_ind = (c_ind + 1) / (c_ind.sum(axis=1).values[:, None] + len(all_items))
            P_ind = c_ind
        else:
            P_ind = pd.DataFrame(1.0/len(all_items), index=['_'], columns=all_items)

        # paid
        if 'paid' in users.columns:
            c_paid = merged.groupby(['paid', 'feature_id']).size().unstack(fill_value=0)
            c_paid = c_paid.reindex(columns=all_items, fill_value=0)
            c_paid = (c_paid + 1) / (c_paid.sum(axis=1).values[:, None] + len(all_items))
            P_paid = c_paid
        else:
            P_paid = pd.DataFrame(1.0/len(all_items), index=['_'], columns=all_items)

        # freq（使用频率粗分桶）
        if 'usage_freq' in users.columns:
            uf = users.copy()
            uf['freq_bin'] = pd.qcut(uf['usage_freq'].fillna(0.0), q=min(4, max(1, uf['usage_freq'].nunique())), duplicates='drop')
            merged2 = logs[['user_id', 'feature_id']].merge(uf[['user_id', 'freq_bin']], on='user_id', how='left')
            c_freq = merged2.groupby(['freq_bin', 'feature_id']).size().unstack(fill_value=0)
            c_freq = c_freq.reindex(columns=all_items, fill_value=0)
            c_freq = (c_freq + 1) / (c_freq.sum(axis=1).values[:, None] + len(all_items))
            P_freq = c_freq
        else:
            P_freq = pd.DataFrame(1.0/len(all_items), index=['_'], columns=all_items)

        # 组合成用户级先验（简单等权，可学习权重）
        prior = []
        for _, row in users.set_index('user_id').iterrows():
            p = np.ones(len(all_items), dtype=float) / len(all_items)
            if 'industry' in users.columns and not pd.isna(row.get('industry')):
                p *= P_ind.loc[row['industry']].values if row['industry'] in P_ind.index else p
            if 'paid' in users.columns and not pd.isna(row.get('paid')):
                p *= P_paid.loc[row['paid']].values if row['paid'] in P_paid.index else p
            if 'usage_freq' in users.columns and not pd.isna(row.get('usage_freq')):
                # 找 freq_bin
                val = row['usage_freq']
                # 最近邻映射：用最接近的分箱
                idx = (P_freq.index.categories if hasattr(P_freq.index, 'categories') else P_freq.index)
                chosen = None
                for b in idx:
                    try:
                        if val >= b.left and val <= b.right:
                            chosen = b
                            break
                    except Exception:
                        pass
                if chosen is not None and chosen in P_freq.index:
                    p *= P_freq.loc[chosen].values
            # 归一化
            p = p / (p.sum() + 1e-9)
            prior.append(p)
        self.content_prior = np.vstack(prior) if len(prior) else np.zeros((n_users, n_items))

        # Bandit 参数（功能级）
        #   用“是否被使用”当作成功/失败的 Bernoulli 统计
        item_clicks = logs.groupby('feature_id').size().reindex(all_items).fillna(0).astype(int).values
        if 'success' in logs.columns:
            item_success = logs.groupby('feature_id')['success'].sum().reindex(all_items).fillna(0).astype(int).values
            item_fail = item_clicks - item_success
        else:
            item_success = item_clicks
            item_fail = np.zeros_like(item_success)
        self.bandit_alpha = self.cfg.bandit_alpha + item_success
        self.bandit_beta = self.cfg.bandit_beta + np.maximum(item_fail, 0)

    # ---------- 打分 ----------
    def _scores_itemcf(self, uidx: int) -> np.ndarray:
        r_u = self.R.getrow(uidx)                    # 1 x n_items
        s = r_u.dot(self.S_item).A1                 # (n_items,)
        return s

    def _scores_usercf(self, uidx: int) -> np.ndarray:
        s_u = self.S_user.getrow(uidx)              # 1 x n_users
        s = s_u.dot(self.R).A1                      # (n_items,)
        return s

    def _scores_content(self, uidx: int) -> np.ndarray:
        if self.content_prior is None or self.content_prior.shape[0] <= uidx:
            return np.zeros(len(self.idx2item))
        return self.content_prior[uidx]

    def _scores_pop(self) -> np.ndarray:
        return self.pop_vec if self.pop_vec is not None else np.zeros(len(self.idx2item))

    def _bandit_sample(self, k: int) -> List[int]:
        # 汤普森采样：从 Beta(alpha_i, beta_i) 采样，取 Top-K
        draws = np.random.beta(self.bandit_alpha, self.bandit_beta)
        return np.argpartition(draws, -k)[-k:]

    # ---------- 公共接口 ----------
    def recommend_for_user(self, user_id: str, k: int = 10, exclude_seen=True, recent_item_ids: Optional[List[str]] = None) -> List[Tuple[str, float]]:
        if user_id not in self.user2idx:
            # 冷启动：用全局/人群先验 + Bandit 探索 + 流行度
            n_items = len(self.item2idx)
            # 简化：全局 prior = pop
            scores = self._scores_pop().copy()
            # 无画像时均匀；有画像可从 content_prior 计算（此处新用户没有索引，跳过）
            explore = self._bandit_sample(k=min(5, n_items))
            scores[explore] += 0.05  # 给探索一点提升
            # Top-K
            top_idx = np.argpartition(scores, -k)[-k:]
            top_idx = top_idx[np.argsort(-scores[top_idx])]
            return [(self.idx2item[i], float(scores[i])) for i in top_idx]

        uidx = self.user2idx[user_id]
        s_item = self._scores_itemcf(uidx)
        s_user = self._scores_usercf(uidx)
        s_cont = self._scores_content(uidx)
        s_pop  = self._scores_pop()

        s = (self.cfg.weight_itemcf * s_item +
             self.cfg.weight_usercf * s_user +
             self.cfg.weight_content * s_cont +
             self.cfg.weight_pop * s_pop)

        # 过滤已大量使用的功能（可按权重阈值）
        if exclude_seen:
            used_cols = self.R.getrow(uidx).nonzero()[1]
            s[used_cols] *= 0.2  # 降权而非强过滤，避免只剩冷门

        # 如提供最近点击功能，可加强邻居（会话级 bias）
        if recent_item_ids:
            for fid in recent_item_ids:
                if fid in self.item2idx:
                    j = self.item2idx[fid]
                    neigh = self.S_item.getrow(j).A1
                    s += 0.1 * neigh

        # 归一化并 Top-K
        s = (s - s.min()) / (s.ptp() + 1e-9)
        top = np.argpartition(s, -k)[-k:]
        top = top[np.argsort(-s[top])]
        return [(self.idx2item[i], float(s[i])) for i in top]

    def starter_pack3(self, user_id: str) -> List[Tuple[str, float]]:
        # 首次引导的 3 个：内容先验 + 流行度，再 MMR 多样化
        if user_id in self.user2idx:
            uidx = self.user2idx[user_id]
            base = 0.6 * self._scores_content(uidx) + 0.4 * self._scores_pop()
        else:
            base = self._scores_pop()
        base = (base - base.min()) / (base.ptp() + 1e-9)
        scores = {self.idx2item[i]: float(base[i]) for i in range(len(base))}
        cands = sorted(scores.keys(), key=lambda x: -scores[x])
        item2cat = self.item_category
        selected = mmr_diversify(cands, scores, item2cat, k=3, lam=self.cfg.mmr_lambda)
        return [(fid, scores[fid]) for fid in selected]

    def similar_features(self, feature_id: str, k: int = 5) -> List[Tuple[str, float]]:
        if feature_id not in self.item2idx:
            return []
        j = self.item2idx[feature_id]
        row = self.S_item.getrow(j).A1
        if row.sum() == 0:
            return []
        top = np.argpartition(row, -k)[-k:]
        top = top[np.argsort(-row[top])]
        return [(self.idx2item[i], float(row[i])) for i in top if i != j]

    # 简化的套餐推荐（示例）
    def recommend_plan(self, user_id: str, plan_catalog: pd.DataFrame, lookback_days: int = 30) -> Tuple[str, pd.DataFrame]:
        """
        plan_catalog: DataFrame [plan, price, quota, overage_price, include_paid_only(bool)]
        基于近30天调用量（用 R 权重近似），选择期望成本最低的套餐。你们可将 quota/权限对应真实套餐。
        """
        if user_id not in self.user2idx:
            # 未知用户：以人群均值估计
            usage = float(np.asarray(self.R.sum(axis=0)).ravel().mean())
        else:
            uidx = self.user2idx[user_id]
            # 以权重近似调用量（也可外部传入真实次数）
            usage = float(self.R.getrow(uidx).sum())

        rows = []
        for _, r in plan_catalog.iterrows():
            quota = float(r['quota'])
            price = float(r['price'])
            over_price = float(r.get('overage_price', 0.0))
            if usage <= quota:
                cost = price
            else:
                cost = price + (usage - quota) * over_price
            rows.append((r['plan'], cost))
        df = pd.DataFrame(rows, columns=['plan', 'expected_cost']).sort_values('expected_cost')
        return df.iloc[0]['plan'], df

    # ---------- 离线评估 ----------
    def evaluate_leave_one_out(self, logs: pd.DataFrame, K: int = 5) -> Dict[str, float]:
        # 对每个用户：最后一次功能使用作为测试
        logs = logs.copy()
        logs['user_id'] = logs['user_id'].astype(str)
        logs['feature_id'] = logs['feature_id'].astype(str)
        to_datetime_col(logs, 'created_at')
        logs = logs.sort_values(['user_id', 'created_at'])
        last = logs.groupby('user_id').tail(1)
        users = last['user_id'].unique().tolist()

        hits, mrr, ndcg, total = 0.0, 0.0, 0.0, 0
        for u in users:
            gt_item = last[last['user_id'] == u]['feature_id'].iloc[0]
            recs = self.recommend_for_user(u, k=K, exclude_seen=True)
            cand = [x[0] for x in recs]
            if gt_item in cand:
                idx = cand.index(gt_item)
                hits += 1
                mrr += 1.0 / (idx + 1)
                ndcg += 1.0 / math.log2(idx + 2)
            total += 1
        return {
            'HitRate@{}'.format(K): hits / (total + 1e-9),
            'MRR@{}'.format(K): mrr / (total + 1e-9),
            'NDCG@{}'.format(K): ndcg / (total + 1e-9),
            'Users': total
        }

# -----------------------------
# A/B 两比例 Z 检验（示例）
# -----------------------------
def proportions_ztest(success_a, total_a, success_b, total_b):
    """返回 z 值与双侧 p 值。"""
    p_pool = (success_a + success_b) / (total_a + total_b + 1e-12)
    diff = success_b/ (total_b + 1e-12) - success_a / (total_a + 1e-12)
    se = math.sqrt(p_pool * (1 - p_pool) * (1.0/ (total_a+1e-12) + 1.0/ (total_b+1e-12)))
    z = diff / (se + 1e-12)
    # 正态分布双尾 p 值
    p = math.erfc(abs(z) / math.sqrt(2))
    return z, p

# -----------------------------
# 使用示例（伪造/或替换为你们真实数据）
# -----------------------------
if __name__ == "__main__":
    # 1) 准备数据（替换为你们的真实 DataFrame）
    # logs: user_id, feature_id, created_at, success(可选)
    # users: user_id, industry, paid, usage_freq
    # features: feature_id, category, requires_paid
    rng = np.random.default_rng(42)
    n_users = 500
    features_list = [f"F{i:02d}" for i in range(1, 25)]
    cats = ['抠图', '换装', '去水印', '修复', '批量', 'API']
    feat_df = pd.DataFrame({
        'feature_id': features_list,
        'category': [random.choice(cats) for _ in features_list],
        'requires_paid': [rng.choice([0,1], p=[0.7,0.3]) for _ in features_list]
    })

    inds = ['电商', '工作室', '广告', '跨境', '自媒体']
    users_df = pd.DataFrame({
        'user_id': [f"U{i:05d}" for i in range(n_users)],
        'industry': [random.choice(inds) for _ in range(n_users)],
        'paid': [rng.choice([0,1], p=[0.7,0.3]) for _ in range(n_users)],
        'usage_freq': rng.integers(0, 50, size=n_users)
    })

    # 日志：每用户 20~80 条
    rows = []
    now = pd.Timestamp.utcnow().normalize()
    for u in users_df['user_id']:
        m = rng.integers(20, 81)
        # 用户偏好若干类别
        pref_cats = rng.choice(cats, size=2, replace=False)
        my_items = feat_df[feat_df['category'].isin(pref_cats)]['feature_id'].tolist()
        for _ in range(m):
            fid = rng.choice(my_items if rng.random() < 0.7 else features_list)
            dt = now - pd.Timedelta(days=int(rng.integers(0, 30)))
            succ = rng.choice([0,1], p=[0.2,0.8])
            rows.append((u, fid, dt, succ))
    logs_df = pd.DataFrame(rows, columns=['user_id','feature_id','created_at','success'])

    # 2) 训练
    cfg = RecoConfig()
    model = HybridCFRecommender(cfg)
    model.fit(logs_df, users_df, feat_df)

    # 3) 推理：给某个用户推荐
    test_uid = users_df['user_id'].iloc[0]
    recs = model.recommend_for_user(test_uid, k=10, exclude_seen=True)
    print("Top-10 推荐：", recs)

    # 新手引导（首屏 3 个）
    print("Starter 3：", model.starter_pack3(test_uid))

    # 相似功能（比如用户用过的某个功能）
    a_feature = feat_df['feature_id'].iloc[0]
    print("与 {} 相似的功能：".format(a_feature), model.similar_features(a_feature, k=5))

    # 套餐推荐（示例）
    plan_catalog = pd.DataFrame([
        {'plan':'Basic', 'price':49, 'quota':100, 'overage_price':0.8},
        {'plan':'Pro',   'price':199,'quota':1000,'overage_price':0.5},
        {'plan':'Ent',   'price':599,'quota':1e9, 'overage_price':0.0},
    ])
    plan, table = model.recommend_plan(test_uid, plan_catalog)
    print("推荐套餐：", plan)
    print(table)

    # 4) 离线评估
    metrics = model.evaluate_leave_one_out(logs_df, K=5)
    print("离线指标：", metrics)

    # 5) A/B 检验（演示）
    #   假设对照组 CTR=0.18, n=10000；实验组 CTR=0.195, n=10000
    z, p = proportions_ztest(1800, 10000, 1950, 10000)
    print("AB Z={}, p={}".format(round(z,3), round(p,5)))
```

---

## 9. 如何把代码接到线上（关键实现点）

1. **离线预计算任务**（Airflow/Dagster）

   * 每 1 小时/天：重建 $R$、更新 $S_{\text{item}}$/$S_{\text{user}}$、人群先验表、流行度、Bandit α/β
   * 落地到 **Redis**（Key 模式：`item:neigh:<fid>`、`user:prior:<uid>`、`stats:bandit` 等）
2. **在线服务**（Python/FastAPI 或 gRPC）

   * 启动加载轻量参数（或全量从 Redis 拉取）
   * **接口**：`/recommend?user_id=...&k=...`；**相似功能**：`/similar?feature_id=...`；**新手引导**：`/starter3?user_id=...`
   * **鉴权**：过滤出**可用功能**（例如未付费禁用付费专属）
3. **日志与特征闭环**

   * 曝光/点击/完成/失败 → Kafka → DWH（Snowflake/BigQuery）→ 次日/小时级再训练
   * Bandit 在线参数可**增量**更新（每 N 分钟聚合一次写回 Redis）
4. **延迟预算**

   * Redis GET/HMGET（1–3ms）×2\~3
   * 内存向量点乘/重排（<2ms）
   * MMR/过滤（1–3ms）
   * 总计 < 20–30ms（跨区 < 50ms）

---

## 10. 新手引导 & 相似功能 & 套餐推荐（业务落地细节）

* **新手引导（首屏 3 个）**

  * 取 `starter_pack3`，再加**文案模板**（行业化）：

    * 电商：`抠图`→“一键白底主图”、`批量换装`→“多款试穿”、`去水印`→“复用素材”
* **相似功能推荐**

  * 前端在功能卡片底部展示：“你可能还需要：换装 / 批量抠图 / 高清修复”，点击即二跳
* **套餐推荐**

  * 以近 30 天用量/成功率估算**性价比**；若预计下月超额>15%，在结算页**主动建议升级**（对比成本明细）

---

## 11. 上线检查清单（截要）

* [ ] Redis/特征/模型 Key 规范与 TTL
* [ ] 冷启动灰度：新用户首屏只走先验+Bandit，逐步引入 CF
* [ ] 权限过滤（未付费不展示“付费专属”）
* [ ] 监控：接口 P95 延迟、推荐点击率、功能成功率、崩溃率
* [ ] A/B 实验埋点与报表（自动计算 Z 值与 p 值）

---

如果你愿意，我可以把上述 Python 脚本拆成\*\*训练脚本 + 在线推理服务（FastAPI）\*\*的两份代码，并给出 Redis Schema & 示例请求/响应 JSON，方便你们直接对接到现有后端。
