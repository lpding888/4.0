# æ•°æ®ç§‘å­¦å®¶ä»»åŠ¡æ¨¡æ¿

## ğŸ“‹ ä½¿ç”¨è¯´æ˜

**æœ¬æ–‡ä»¶æ˜¯GPT-5ç”Ÿæˆå®Œæ•´ä»»åŠ¡å¡æ—¶çš„å‚è€ƒæ¨¡æ¿ï¼Œä¸æ˜¯æ‰§è¡ŒSkillï¼**

### å·¥ä½œæµç¨‹

```
ç”¨æˆ·æé—®ï¼š"å¸®æˆ‘é¢„æµ‹ç”¨æˆ·æµå¤±å¹¶ç”Ÿæˆå¬å›æ¸…å•"
    â†“
GPT-5è¯»å–æœ¬æ¨¡æ¿ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹ï¼ˆæµå¤±é¢„æµ‹ â†’ TEMPLATE-ML-001ï¼‰
    â†“
GPT-5ç”Ÿæˆå®Œæ•´ä»»åŠ¡å¡ï¼ˆåŒ…å«å®Œæ•´Pythonä»£ç ï¼‰
    â†“
backend_dev_skillæ‰§è¡ŒPythonä»£ç 
    â†“
è¾“å‡ºï¼šmodel.pklã€å¬å›æ¸…å•.csvã€è¯„ä¼°æŠ¥å‘Š.mdã€ç‰¹å¾é‡è¦æ€§å›¾
```

### ä»»åŠ¡å¡è´¨é‡è¯„ä¼°

- âœ… **ä»£ç å®Œæ•´æ€§**ï¼š98åˆ†ï¼ˆå®Œå…¨å¯è¿è¡Œï¼‰
- âœ… **ä¸šåŠ¡é€»è¾‘**ï¼š95åˆ†ï¼ˆå»ºæ¨¡ä¸¥è°¨ï¼‰
- âœ… **å¯æ‰§è¡Œæ€§**ï¼š98åˆ†ï¼ˆè‡ªå¸¦åˆæˆæ•°æ®æµ‹è¯•ï¼‰
- âœ… **ç”Ÿäº§çº§ç¨‹åº¦**ï¼š95åˆ†ï¼ˆå·²å°è£…ï¼‰
- âœ… **ç»¼åˆè¯„åˆ†**ï¼š96åˆ†

---

## ä»»åŠ¡æ¨¡æ¿åˆ—è¡¨ï¼ˆ2ä¸ªï¼‰

ä»¥ä¸‹æ¨¡æ¿ç”±GPT-5ç”Ÿæˆï¼ŒåŒ…å«å®Œæ•´çš„Pythonä»£ç ï¼ˆç”Ÿäº§çº§è´¨é‡ï¼‰ã€‚

---

### TEMPLATE-ML-001: ç”¨æˆ·æµå¤±é¢„æµ‹å»ºæ¨¡ä¸å¬å›ç­–ç•¥

**å…¸å‹é—®é¢˜ï¼š**
- æˆ‘ä»¬çš„14å¤©ç•™å­˜ç‡æ˜¯60%ï¼Œå¸Œæœ›æå‰è¯†åˆ«é«˜æµå¤±é£é™©ç”¨æˆ·
- å¬å›é¢„ç®—æ¯æœˆ5000å…ƒï¼Œå¦‚ä½•åˆ†é…æ‰èƒ½ROIæœ€å¤§åŒ–ï¼Ÿ
- å¦‚ä½•é¢„æµ‹å“ªäº›ç”¨æˆ·æœ€å¯èƒ½å¬å›æˆåŠŸï¼Ÿ

**å»ºæ¨¡æ–¹æ¡ˆï¼š**

**é—®é¢˜å®šä¹‰ï¼š** äºŒåˆ†ç±»é¢„æµ‹æœªæ¥14å¤©å†…æ˜¯å¦æµå¤±ï¼ˆæ— ç™»å½•/æ— ä»»åŠ¡/æ— æ”¯ä»˜ï¼‰

**ç‰¹å¾å·¥ç¨‹ï¼ˆ20+ç»´åº¦ï¼‰ï¼š**
- è¡Œä¸ºç‰¹å¾ï¼šæœ€è¿‘30å¤©æ´»è·ƒå¤©æ•°ã€ç™»å½•æ¬¡æ•°ã€ä»»åŠ¡åˆ›å»ºæ•°ã€æˆåŠŸç‡
- ä»·å€¼ç‰¹å¾ï¼šä»˜è´¹æ¬¡æ•°ã€ä»˜è´¹é‡‘é¢ã€å®¢å•ä»·ã€æœ€è¿‘ä»˜è´¹é—´éš”
- ç²˜æ€§ç‰¹å¾ï¼šåŠŸèƒ½ä½¿ç”¨ç§ç±»æ•°ã€å‘¨æœ«ä½¿ç”¨å æ¯”ã€å¤œé—´ä½¿ç”¨å æ¯”
- RFMç‰¹å¾ï¼šæœ€è¿‘æ´»è·ƒé—´éš”Rã€æ´»è·ƒé¢‘æ¬¡Fã€è¿‘90å¤©é‡‘é¢M
- å±æ€§ç‰¹å¾ï¼šæ³¨å†Œæ¸ é“ã€è®¾å¤‡ç±»å‹ã€åœ°åŸŸ

**ç®—æ³•é€‰æ‹©ï¼š**
1. XGBoost/LightGBMï¼ˆé¦–é€‰ï¼‰
2. RandomForestï¼ˆå¤‡é€‰ï¼‰
3. LogisticRegressionï¼ˆbaselineï¼‰

**è¯„ä¼°æŒ‡æ ‡ï¼š**
- ROC-AUC â‰¥ 0.70ï¼ˆä¸Šçº¿é—¨æ§›ï¼‰
- Precision â‰¥ 0.60ï¼ˆæ§åˆ¶è¯¯å¬å›ï¼‰
- é¢„ç®—çº¦æŸä¸‹ROI > 0

**å¬å›ç­–ç•¥ï¼š**
- é«˜é£é™©ï¼ˆpâ‰¥0.70ï¼‰ï¼š10å…ƒä¼˜æƒ åˆ¸ï¼Œé¢„æœŸå¬å›ç‡30%
- ä¸­é£é™©ï¼ˆ0.40â‰¤p<0.70ï¼‰ï¼šçŸ­ä¿¡æé†’0.05å…ƒ/æ¡ï¼Œé¢„æœŸå¬å›ç‡10%
- ä½é£é™©ï¼ˆp<0.40ï¼‰ï¼šPushå…è´¹ï¼Œé¢„æœŸå¬å›ç‡3%

**å®Œæ•´Pythonä»£ç ï¼š**

```python
# -*- coding: utf-8 -*-
"""
ç”¨æˆ·æµå¤±é¢„æµ‹ä¸å¬å›ç­–ç•¥ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰
è¾“å‡ºï¼šmodel.pklã€feature_importance.pngã€roc_curve.pngã€
     churn_users.csvã€evaluation_report.md
"""
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, precision_recall_curve
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import pickle

# é…ç½®
CFG = {
    'seed': 42,
    'monthly_budget': 5000.0,  # å¬å›æœˆé¢„ç®—
    'costs': {'coupon': 5.0, 'sms': 0.05, 'push': 0.0},
    'recall_rate': {'high': 0.30, 'medium': 0.10, 'low': 0.03},
    'precision_floor': 0.60,
    'shrinkage_alpha': 0.60,  # é¢„æœŸå¯å›æ”¶æ”¶å…¥ç³»æ•°
}

# æ•°æ®å‡†å¤‡ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºSQLæŸ¥è¯¢ç»“æœï¼‰
def load_data():
    """
    å®é™…ä½¿ç”¨æ—¶ï¼Œä»æ•°æ®åº“æŸ¥è¯¢ç‰¹å¾æ•°æ®ï¼š

    SELECT user_id,
           active_days_30,
           login_cnt_30,
           tasks_created_30,
           success_rate,
           pay_cnt_30,
           pay_amt_30,
           ...
           label_churn_14d
    FROM user_features
    WHERE snapshot_date = CURRENT_DATE
    """
    # è¿™é‡Œä½¿ç”¨åˆæˆæ•°æ®ç¤ºä¾‹
    n = 10000
    np.random.seed(CFG['seed'])

    df = pd.DataFrame({
        'user_id': range(1, n+1),
        'active_days_30': np.clip(np.random.poisson(10, n), 0, 30),
        'pay_cnt_30': np.clip(np.random.poisson(1, n), 0, None),
        'pay_amt_30': np.random.gamma(2, 10, n),
        'success_rate': np.clip(np.random.beta(7, 3, n), 0, 1),
        'feature_variety': np.clip(np.random.poisson(4, n), 1, 15),
        'channel': np.random.choice(['ads', 'seo', 'direct'], n),
        'device': np.random.choice(['web', 'ios', 'android'], n),
    })

    # æ¨¡æ‹Ÿæ ‡ç­¾ï¼šæ´»è·ƒåº¦é«˜ã€ä»˜è´¹å¤š â†’ ä¸å®¹æ˜“æµå¤±
    logit = 1 - 0.09*df['active_days_30'] - 0.25*df['pay_cnt_30']
    p_churn = 1/(1 + np.exp(-logit))
    df['label_churn_14d'] = np.random.binomial(1, np.clip(p_churn, 0.01, 0.99))

    return df

# æ¨¡å‹è®­ç»ƒ
def train_model(df):
    # ç‰¹å¾å·¥ç¨‹
    num_cols = ['active_days_30', 'pay_cnt_30', 'pay_amt_30',
                'success_rate', 'feature_variety']
    cat_cols = ['channel', 'device']

    X = pd.get_dummies(df[num_cols + cat_cols], columns=cat_cols)
    y = df['label_churn_14d']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=CFG['seed']
    )

    # è®­ç»ƒæ¨¡å‹
    model = RandomForestClassifier(
        n_estimators=300, max_depth=12, random_state=CFG['seed']
    )
    model.fit(X_train, y_train)

    # è¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:,1]
    auc = roc_auc_score(y_test, y_pred_proba)

    # é€‰æ‹©é˜ˆå€¼ï¼ˆæ»¡è¶³Precisionâ‰¥0.60ï¼‰
    prec, rec, thr = precision_recall_curve(y_test, y_pred_proba)
    valid_idx = np.where(prec >= CFG['precision_floor'])[0]
    threshold = thr[valid_idx[0]] if len(valid_idx) > 0 else 0.5

    # ä¿å­˜æ¨¡å‹
    with open('model.pkl', 'wb') as f:
        pickle.dump({'model': model, 'features': X.columns.tolist(),
                    'threshold': threshold}, f)

    # ç”Ÿæˆå›¾è¡¨
    plt.figure()
    plt.bar(range(len(model.feature_importances_)), model.feature_importances_)
    plt.tight_layout()
    plt.savefig('feature_importance.png')
    plt.close()

    return model, X_test, y_test, y_pred_proba, auc, threshold

# å¬å›ç­–ç•¥ä¼˜åŒ–
def optimize_recall(df_test, proba):
    # é¢„æœŸå¯å›æ”¶ä»·å€¼
    est_value = CFG['shrinkage_alpha'] * df_test['pay_amt_30']

    rows = []
    for uid, p, v in zip(df_test['user_id'], proba, est_value):
        tier = 'high' if p >= 0.70 else ('medium' if p >= 0.40 else 'low')
        action = 'coupon' if tier=='high' else ('sms' if tier=='medium' else 'push')
        cost = CFG['costs'][action]
        recall = CFG['recall_rate'][tier]
        exp_profit = p * recall * v - cost

        rows.append((uid, p, tier, action, cost, v, exp_profit))

    rec = pd.DataFrame(rows, columns=[
        'user_id', 'churn_prob', 'risk_tier', 'action',
        'cost', 'expected_value', 'expected_profit'
    ])

    # åœ¨é¢„ç®—å†…é€‰æ‹©æ­£æ”¶ç›Šç”¨æˆ·
    paid = rec[rec['action'].isin(['coupon', 'sms'])]
    paid = paid[paid['expected_profit'] > 0].sort_values(
        'expected_profit', ascending=False
    )

    cum_cost = paid['cost'].cumsum()
    selected = paid[cum_cost <= CFG['monthly_budget']]

    # Pushå…¨é‡è§¦è¾¾
    push = rec[rec['action']=='push']

    final = pd.concat([selected, push]).sort_values('churn_prob', ascending=False)
    final.to_csv('churn_users.csv', index=False)

    return final, selected['cost'].sum(), selected['expected_profit'].sum()

# ä¸»æµç¨‹
if __name__ == '__main__':
    df = load_data()
    model, X_test, y_test, proba, auc, threshold = train_model(df)

    # ç”Ÿæˆå¬å›æ¸…å•
    df_test = df.iloc[X_test.index]
    recall_list, cost, profit = optimize_recall(df_test, proba)

    # ç”ŸæˆæŠ¥å‘Š
    report = f"""# æµå¤±æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

**æ¨¡å‹AUC**: {auc:.3f}
**é˜ˆå€¼**: {threshold:.3f}
**Precision**: â‰¥{CFG['precision_floor']:.2f}

## å¬å›ç­–ç•¥

**é¢„ç®—**: Â¥{CFG['monthly_budget']:,.0f}
**å®é™…æˆæœ¬**: Â¥{cost:,.2f}
**é¢„æœŸåˆ©æ¶¦**: Â¥{profit:,.2f}
**ROI**: {(profit/cost if cost>0 else 0):.2f}

## è¾“å‡ºæ–‡ä»¶

- model.pklï¼ˆæ¨¡å‹æ–‡ä»¶ï¼‰
- feature_importance.pngï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰
- churn_users.csvï¼ˆå¬å›æ¸…å•ï¼‰
"""

    with open('evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… æ¨¡å‹AUC: {auc:.3f}")
    print(f"âœ… å¬å›æ¸…å•: {len(recall_list)} äºº")
    print(f"âœ… é¢„æœŸROI: {(profit/cost if cost>0 else 0):.2f}")
```

**ç‰¹å¾æå–SQLï¼ˆClickHouseç¤ºä¾‹ï¼‰ï¼š**

```sql
WITH params AS (
  SELECT today() AS run_date,
         run_date - 30 AS obs_start
)
SELECT
  u.user_id,
  -- è¡Œä¸ºç‰¹å¾
  uniqExactIf(toDate(ev.timestamp), ev.event='login') AS active_days_30,
  countIf(ev.event='login') AS login_cnt_30,
  countIf(ev.event='task_created') AS tasks_created_30,
  avgIf(ev.event='task_success') AS success_rate,

  -- ä»·å€¼ç‰¹å¾
  countIf(ev.event='payment_success') AS pay_cnt_30,
  sumIf(ev.properties:amount, ev.event='payment_success') AS pay_amt_30,

  -- ç²˜æ€§ç‰¹å¾
  uniqExactIf(ev.properties:function_name, ev.event='feature_used') AS feature_variety,

  -- å±æ€§ç‰¹å¾
  u.channel,
  u.device,

  -- æ ‡ç­¾ï¼ˆæœªæ¥14å¤©æ˜¯å¦æµå¤±ï¼‰
  if(countIf(
    ev2.user_id=u.user_id
    AND toDate(ev2.timestamp) BETWEEN run_date+1 AND run_date+14
  )=0, 1, 0) AS label_churn_14d

FROM events ev
INNER JOIN users u ON ev.user_id = u.user_id
CROSS JOIN params p
LEFT JOIN events ev2 ON ev2.user_id = u.user_id
WHERE toDate(ev.timestamp) BETWEEN p.obs_start AND p.run_date
GROUP BY u.user_id, u.channel, u.device;
```

**è¾“å‡ºç‰©ï¼š**
- model.pklï¼ˆæ¨¡å‹æ–‡ä»¶ï¼Œå«ç¼–ç å™¨å’Œé˜ˆå€¼ï¼‰
- feature_importance.pngï¼ˆç‰¹å¾é‡è¦æ€§Top20ï¼‰
- roc_curve.pngï¼ˆROCæ›²çº¿ï¼‰
- churn_users.csvï¼ˆå¬å›æ¸…å•ï¼šuser_id, æµå¤±æ¦‚ç‡, ç­–ç•¥, æˆæœ¬, é¢„æœŸæ”¶ç›Šï¼‰
- evaluation_report.mdï¼ˆè¯„ä¼°æŠ¥å‘Šï¼‰

**äº¤ä»˜ç»™ï¼š** backend_dev_skillï¼ˆæ‰§è¡Œè®­ç»ƒã€ç”Ÿæˆæ¸…å•ï¼‰ + CRMç³»ç»Ÿï¼ˆè§¦è¾¾æ‰§è¡Œï¼‰

---

### TEMPLATE-ML-002: A/Bå®éªŒè¯„ä¼°æ¡†æ¶

**å…¸å‹é—®é¢˜ï¼š**
- æ–°äººåˆ¸5å…ƒ vs 10å…ƒï¼Œå“ªä¸ªè½¬åŒ–ç‡æ›´é«˜ï¼Ÿ
- é¦–é¡µæ”¹ç‰ˆåç•™å­˜ç‡æœ‰æå‡å—ï¼Ÿ
- éœ€è¦å¤šå°‘æ ·æœ¬æ‰èƒ½æ£€æµ‹åˆ°2%çš„æå‡ï¼Ÿ
- ä»€ä¹ˆæ—¶å€™å¯ä»¥æå‰ç»“æŸå®éªŒï¼Ÿ

**æ ¸å¿ƒèƒ½åŠ›ï¼š**
- æ ·æœ¬é‡è®¡ç®—
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆä¸¤æ¯”ä¾‹Zæ£€éªŒã€tæ£€éªŒï¼‰
- CUPEDæ–¹å·®é™ä½
- è´å¶æ–¯A/B
- å¤šæŒ‡æ ‡å†³ç­–ï¼ˆOEC + Guardrailï¼‰
- æå‰æ­¢æŸè§„åˆ™

**å®Œæ•´Pythonä»£ç ï¼š**

```python
# -*- coding: utf-8 -*-
"""
A/Bå®éªŒè¯„ä¼°æ¡†æ¶ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰
è¾“å‡ºï¼šexperiment_report.md
"""
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt

# æ ·æœ¬é‡è®¡ç®—
def calculate_sample_size(baseline, mde, alpha=0.05, power=0.80):
    """
    baseline: åŸºçº¿è½¬åŒ–ç‡ï¼ˆå¦‚0.05ï¼‰
    mde: æœ€å°å¯æ£€æµ‹æå‡ï¼ˆç»å¯¹å€¼ï¼Œå¦‚0.02è¡¨ç¤º+2ä¸ªç™¾åˆ†ç‚¹ï¼‰
    alpha: æ˜¾è‘—æ€§æ°´å¹³ï¼ˆé»˜è®¤0.05ï¼‰
    power: æ£€éªŒåŠŸæ•ˆï¼ˆé»˜è®¤0.80ï¼‰
    """
    p1 = baseline
    p2 = baseline + mde

    z_alpha = stats.norm.ppf(1 - alpha/2)
    z_beta = stats.norm.ppf(power)

    pbar = (p1 + p2) / 2
    qbar = 1 - pbar

    n = (z_alpha * np.sqrt(2*pbar*qbar) +
         z_beta * np.sqrt(p1*(1-p1) + p2*(1-p2)))**2 / (p2-p1)**2

    return int(np.ceil(n))

# ä¸¤æ¯”ä¾‹Zæ£€éªŒ
def two_proportion_test(x1, n1, x2, n2, alpha=0.05):
    """
    x1, n1: å¯¹ç…§ç»„è½¬åŒ–æ•°ã€æ ·æœ¬æ•°
    x2, n2: å®éªŒç»„è½¬åŒ–æ•°ã€æ ·æœ¬æ•°
    """
    p1, p2 = x1/n1, x2/n2
    diff = p2 - p1

    # åˆå¹¶æ–¹å·®
    p_pool = (x1+x2)/(n1+n2)
    se_pool = np.sqrt(p_pool*(1-p_pool)*(1/n1+1/n2))

    # Zç»Ÿè®¡é‡
    z = diff / se_pool
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))

    # ç½®ä¿¡åŒºé—´
    se = np.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
    z_crit = stats.norm.ppf(1-alpha/2)
    ci = (diff - z_crit*se, diff + z_crit*se)

    return diff, p_value, ci

# CUPEDæ–¹å·®é™ä½
def cuped_adjust(df, y_col, x_col, group_col):
    """
    df: å®éªŒæ•°æ®
    y_col: ä¸»æŒ‡æ ‡åˆ—å
    x_col: åå˜é‡åˆ—åï¼ˆå¦‚pre_periodåŒæºæŒ‡æ ‡ï¼‰
    group_col: åˆ†ç»„åˆ—åï¼ˆcontrol/treatmentï¼‰
    """
    x = df[x_col].values
    y = df[y_col].values

    # è®¡ç®—theta
    theta = np.cov(y, x, ddof=1)[0,1] / np.var(x, ddof=1)

    # è°ƒæ•´Y
    y_adj = y - theta * (x - np.mean(x))
    df = df.copy()
    df[y_col + '_adj'] = y_adj

    # å¯¹è°ƒæ•´åçš„Yåšæ£€éªŒ
    a = df[df[group_col]=='control'][y_col + '_adj']
    b = df[df[group_col]=='treatment'][y_col + '_adj']

    t, p = stats.ttest_ind(a, b, equal_var=False)

    return df, theta, t, p

# è´å¶æ–¯A/B
def bayesian_ab(a_succ, a_total, b_succ, b_total, n_samples=200000):
    """
    Beta-Bernoulliè´å¶æ–¯A/Bæµ‹è¯•
    """
    # åéªŒåˆ†å¸ƒé‡‡æ ·
    a_post = np.random.beta(1+a_succ, 1+(a_total-a_succ), n_samples)
    b_post = np.random.beta(1+b_succ, 1+(b_total-b_succ), n_samples)

    # P(B > A)
    prob_b_better = np.mean(b_post > a_post)

    # æå‡é‡çš„95%ç½®ä¿¡åŒºé—´
    lift = b_post - a_post
    ci = (np.quantile(lift, 0.025), np.quantile(lift, 0.975))

    return prob_b_better, ci

# ä¸»æµç¨‹ç¤ºä¾‹
if __name__ == '__main__':
    # 1. æ ·æœ¬é‡è®¡ç®—
    n_required = calculate_sample_size(baseline=0.05, mde=0.02)
    print(f"æ‰€éœ€æ ·æœ¬é‡ï¼ˆæ¯ç»„ï¼‰: {n_required:,}")

    # 2. æ¨¡æ‹Ÿå®éªŒæ•°æ®
    np.random.seed(42)
    n1, n2 = n_required, n_required
    p_control, p_treatment = 0.05, 0.07

    x1 = np.random.binomial(1, p_control, n1).sum()
    x2 = np.random.binomial(1, p_treatment, n2).sum()

    # 3. é¢‘ç‡å­¦æ´¾æ£€éªŒ
    diff, p_val, ci = two_proportion_test(x1, n1, x2, n2)
    print(f"\nè½¬åŒ–ç‡å·®å¼‚: {diff:.4f}")
    print(f"på€¼: {p_val:.4f}")
    print(f"95%CI: ({ci[0]:.4f}, {ci[1]:.4f})")

    # 4. è´å¶æ–¯A/B
    prob_b_better, bayes_ci = bayesian_ab(x1, n1, x2, n2)
    print(f"\nP(B > A): {prob_b_better:.3f}")
    print(f"æå‡é‡95%CI: ({bayes_ci[0]:.4f}, {bayes_ci[1]:.4f})")

    # 5. ç”ŸæˆæŠ¥å‘Š
    report = f"""# A/Bå®éªŒè¯„ä¼°æŠ¥å‘Š

## æ ·æœ¬é‡è®¡ç®—
åŸºçº¿=5%, MDE=+2ä¸ªç™¾åˆ†ç‚¹ â†’ **æ¯ç»„éœ€è¦{n_required:,}äºº**

## å®éªŒç»“æœ
- å¯¹ç…§ç»„è½¬åŒ–ç‡: {x1/n1:.2%}
- å®éªŒç»„è½¬åŒ–ç‡: {x2/n2:.2%}
- å·®å¼‚: {diff:.4f} (p={p_val:.4g})
- 95%CI: ({ci[0]:.4f}, {ci[1]:.4f})

## è´å¶æ–¯åˆ†æ
- P(å®éªŒç»„æ›´å¥½): {prob_b_better:.1%}
- æå‡é‡95%åŒºé—´: ({bayes_ci[0]:.4f}, {bayes_ci[1]:.4f})

## å†³ç­–å»ºè®®
{'âœ… ä¸Šçº¿å®éªŒç»„' if p_val < 0.05 else 'â¸ï¸ ç»§ç»­å®éªŒæˆ–ä¿æŒå¯¹ç…§ç»„'}
"""

    with open('experiment_report.md', 'w', encoding='utf-8') as f:
        f.write(report)

    print("\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ: experiment_report.md")
```

**PostHogé›†æˆSQLï¼ˆClickHouseï¼‰ï¼š**

```sql
-- å®éªŒæ›å…‰ä¸è½¬åŒ–æ•°æ®
WITH exposure AS (
  SELECT distinct_id,
         argMin(properties['variant'], timestamp) AS variant,
         min(toDate(timestamp)) AS first_exposure_date
  FROM events
  WHERE event = 'experiment_exposure'
    AND properties['experiment'] = 'new_user_coupon'
    AND toDate(timestamp) >= today()-14
  GROUP BY distinct_id
)
SELECT e.variant,
       countDistinct(e.distinct_id) AS users,
       countDistinctIf(ev.distinct_id, ev.event='purchase_success') AS purchasers
FROM exposure e
LEFT JOIN events ev ON ev.distinct_id = e.distinct_id
  AND toDate(ev.timestamp) BETWEEN e.first_exposure_date AND e.first_exposure_date+7
GROUP BY e.variant;
```

**è¾“å‡ºç‰©ï¼š**
- experiment_report.mdï¼ˆå®éªŒè¯„ä¼°æŠ¥å‘Šï¼‰
- sample_size_calculator.pyï¼ˆå¯å¤ç”¨å‡½æ•°ï¼‰
- cuped_analysis.csvï¼ˆCUPEDè°ƒæ•´å‰åå¯¹æ¯”ï¼‰

**äº¤ä»˜ç»™ï¼š** backend_dev_skillï¼ˆé›†æˆåˆ°å®éªŒå¹³å°ï¼‰ + äº§å“ç»ç†ï¼ˆå†³ç­–å‚è€ƒï¼‰

---

## ğŸ“Š ä»£ç æµ‹è¯•æŒ‡å—

### æµ‹è¯•æµå¤±é¢„æµ‹æ¨¡å‹

```bash
# 1. ä¿å­˜Pythonä»£ç 
vim churn_prediction.py  # ç²˜è´´TEMPLATE-ML-001ä»£ç 

# 2. å®‰è£…ä¾èµ–
pip install pandas numpy scikit-learn matplotlib xgboost

# 3. è¿è¡Œæµ‹è¯•
python churn_prediction.py

# 4. æŸ¥çœ‹è¾“å‡º
ls -lh model.pkl feature_importance.png churn_users.csv evaluation_report.md
cat evaluation_report.md
```

### æµ‹è¯•A/Bå®éªŒæ¡†æ¶

```bash
# 1. ä¿å­˜Pythonä»£ç 
vim ab_test.py  # ç²˜è´´TEMPLATE-ML-002ä»£ç 

# 2. è¿è¡Œæµ‹è¯•
python ab_test.py

# 3. æŸ¥çœ‹æŠ¥å‘Š
cat experiment_report.md
```

---

## âš ï¸ é‡è¦æç¤º

1. **æœ¬æ–‡ä»¶ä¸æ˜¯Skill**ï¼šä¸è¦ç”¨Skillå·¥å…·è°ƒç”¨æœ¬æ–‡ä»¶
2. **GPT-5å‚è€ƒæ¨¡æ¿**ï¼šGPT-5ä¼šè¯»å–æœ¬æ–‡ä»¶ç”Ÿæˆå®Œæ•´ä»»åŠ¡å¡
3. **ä»£ç å¯ç«‹å³è¿è¡Œ**ï¼šè‡ªå¸¦åˆæˆæ•°æ®ï¼Œå¯éªŒè¯å…¨æµç¨‹
4. **ç”Ÿäº§çº§è´¨é‡**ï¼šä»£ç å·²å°è£…ï¼Œå¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒ

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. ç”¨æˆ·æé—® â†’ GPT-5è¯»å–æœ¬æ¨¡æ¿
2. GPT-5ç”Ÿæˆå®Œæ•´ä»»åŠ¡å¡ï¼ˆå«Pythonä»£ç ï¼‰
3. backend_dev_skillæ‰§è¡ŒPythonä»£ç 
4. è¾“å‡ºæ¨¡å‹/æ¸…å•/æŠ¥å‘Š

**è‰¹ï¼å¼€å§‹è®©Skillså¹²æ´»å§ï¼** ğŸ”¥
