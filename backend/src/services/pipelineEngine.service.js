const db = require('../config/database');
const logger = require('../utils/logger');
const quotaService = require('./quota.service');

/**
 * PipelineEngine - æ ¸å¿ƒç¼–æ’å¼•æ“
 * è´Ÿè´£æŒ‰ç…§Pipeline Schemaæ‰§è¡Œå¤šæ­¥éª¤ä»»åŠ¡æµç¨‹
 */
class PipelineEngine {
  /**
   * æ‰§è¡ŒPipeline
   * @param {string} taskId - ä»»åŠ¡ID
   * @param {string} featureId - åŠŸèƒ½ID
   * @param {Object} inputData - ç”¨æˆ·è¾“å…¥æ•°æ®
   */
  async executePipeline(taskId, featureId, inputData) {
    try {
      logger.info(`[PipelineEngine] å¼€å§‹æ‰§è¡ŒPipeline taskId=${taskId} featureId=${featureId}`);

      // 1. è·å–åŠŸèƒ½å®šä¹‰å’ŒPipeline Schema
      const feature = await db('feature_definitions')
        .where('feature_id', featureId)
        .first();

      if (!feature || !feature.pipeline_schema_ref) {
        throw new Error('åŠŸèƒ½é…ç½®é”™è¯¯:ç¼ºå°‘pipeline_schema_ref');
      }

      const pipelineSchema = await db('pipeline_schemas')
        .where('pipeline_id', feature.pipeline_schema_ref)
        .first();

      if (!pipelineSchema) {
        throw new Error(`Pipeline Schemaä¸å­˜åœ¨: ${feature.pipeline_schema_ref}`);
      }

      const steps = JSON.parse(pipelineSchema.steps);
      if (!Array.isArray(steps) || steps.length === 0) {
        throw new Error('Pipeline Schema stepsé…ç½®é”™è¯¯');
      }

      // 2. æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºprocessing
      await db('tasks')
        .where('id', taskId)
        .update({
          status: 'processing',
          updated_at: new Date()
        });

      // 3. åˆ›å»ºtask_stepsè®°å½•
      const taskSteps = steps.map((step, index) => ({
        task_id: taskId,
        step_index: index,
        type: step.type,
        provider_ref: step.provider_ref,
        status: 'pending',
        input: JSON.stringify(index === 0 ? inputData : {}), // ç¬¬ä¸€æ­¥ä½¿ç”¨inputData
        created_at: new Date()
      }));

      await db('task_steps').insert(taskSteps);
      logger.info(`[PipelineEngine] åˆ›å»º${steps.length}ä¸ªæ­¥éª¤è®°å½• taskId=${taskId}`);

      // 4. æŒ‰é¡ºåºæ‰§è¡Œå„ä¸ªæ­¥éª¤
      let previousOutput = inputData; // ç¬¬ä¸€æ­¥çš„input

      for (let i = 0; i < steps.length; i++) {
        const step = steps[i];
        const stepConfig = {
          taskId,
          stepIndex: i,
          type: step.type,
          providerRef: step.provider_ref,
          timeout: step.timeout || 30000,
          retryPolicy: step.retry_policy || {}
        };

        logger.info(
          `[PipelineEngine] æ‰§è¡Œæ­¥éª¤${i + 1}/${steps.length} ` +
          `taskId=${taskId} type=${step.type} provider=${step.provider_ref}`
        );

        // æ‰§è¡Œæ­¥éª¤
        const stepResult = await this.executeStep(stepConfig, previousOutput);

        if (!stepResult.success) {
          // æ­¥éª¤å¤±è´¥,ç»ˆæ­¢Pipeline
          await this.handlePipelineFailure(
            taskId,
            featureId,
            i,
            stepResult.error
          );
          return;
        }

        // æ­¥éª¤æˆåŠŸ,è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
        previousOutput = stepResult.output;
      }

      // 5. æ‰€æœ‰æ­¥éª¤æˆåŠŸ,æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºsuccess
      await this.handlePipelineSuccess(taskId, previousOutput);

      logger.info(`[PipelineEngine] Pipelineæ‰§è¡ŒæˆåŠŸ taskId=${taskId}`);

    } catch (error) {
      logger.error(
        `[PipelineEngine] Pipelineæ‰§è¡Œå¼‚å¸¸ taskId=${taskId} error=${error.message}`,
        { taskId, featureId, error }
      );

      // å¤„ç†å¼‚å¸¸
      await this.handlePipelineFailure(taskId, featureId, -1, error.message);
    }
  }

  /**
   * æ‰§è¡Œå•ä¸ªæ­¥éª¤
   * @param {Object} stepConfig - æ­¥éª¤é…ç½®
   * @param {Object} input - è¾“å…¥æ•°æ®
   * @returns {Promise<Object>} {success, output, error}
   */
  async executeStep(stepConfig, input) {
    const { taskId, stepIndex, type, providerRef, timeout, retryPolicy } = stepConfig;

    try {
      // æ›´æ–°æ­¥éª¤çŠ¶æ€ä¸ºprocessing
      await db('task_steps')
        .where({ task_id: taskId, step_index: stepIndex })
        .update({
          status: 'processing',
          input: JSON.stringify(input),
          started_at: new Date()
        });

      // æ ¹æ®typeè°ƒç”¨å¯¹åº”çš„provider
      let provider;
      try {
        provider = this.getProvider(type, providerRef);
      } catch (error) {
        logger.error(`[PipelineEngine] ProvideråŠ è½½å¤±è´¥ type=${type} ref=${providerRef}`);
        throw error;
      }

      // æ‰§è¡Œprovider(å¸¦é‡è¯•æœºåˆ¶)
      const maxRetries = retryPolicy.max_retries || 0;
      const retryDelay = retryPolicy.retry_delay_ms || 1000;

      let lastError;
      for (let attempt = 0; attempt <= maxRetries; attempt++) {
        try {
          if (attempt > 0) {
            logger.info(
              `[PipelineEngine] é‡è¯•æ­¥éª¤ attempt=${attempt}/${maxRetries} ` +
              `taskId=${taskId} stepIndex=${stepIndex}`
            );
            await this.sleep(retryDelay);
          }

          const output = await Promise.race([
            provider.execute(input, taskId),
            this.timeout(timeout, `æ­¥éª¤æ‰§è¡Œè¶…æ—¶(${timeout}ms)`)
          ]);

          // æˆåŠŸ,æ›´æ–°æ­¥éª¤çŠ¶æ€
          await db('task_steps')
            .where({ task_id: taskId, step_index: stepIndex })
            .update({
              status: 'completed',
              output: JSON.stringify(output),
              completed_at: new Date()
            });

          return { success: true, output };

        } catch (error) {
          lastError = error;
          logger.warn(
            `[PipelineEngine] æ­¥éª¤æ‰§è¡Œå¤±è´¥ attempt=${attempt} ` +
            `taskId=${taskId} stepIndex=${stepIndex} error=${error.message}`
          );
        }
      }

      // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
      throw lastError;

    } catch (error) {
      // æ›´æ–°æ­¥éª¤çŠ¶æ€ä¸ºfailed
      await db('task_steps')
        .where({ task_id: taskId, step_index: stepIndex })
        .update({
          status: 'failed',
          error_message: error.message,
          completed_at: new Date()
        });

      return {
        success: false,
        error: error.message
      };
    }
  }

  /**
   * è·å–Providerå®ä¾‹
   * @param {string} type - Providerç±»å‹
   * @param {string} providerRef - Providerå¼•ç”¨
   * @returns {Object} Providerå®ä¾‹
   */
  getProvider(type, providerRef) {
    // æ ¹æ®typeåŠ¨æ€åŠ è½½provideræ¨¡å—
    // ä¾‹å¦‚: SYNC_IMAGE_PROCESS -> ./providers/syncImageProcess.provider.js

    const providerMap = {
      'SYNC_IMAGE_PROCESS': './providers/syncImageProcess.provider',
      'RUNNINGHUB_WORKFLOW': './providers/runninghubWorkflow.provider',
      'SCF_POST_PROCESS': './providers/scfPostProcess.provider'
    };

    const providerPath = providerMap[type];
    if (!providerPath) {
      throw new Error(`æœªçŸ¥çš„Providerç±»å‹: ${type}`);
    }

    try {
      const ProviderClass = require(providerPath);
      return new ProviderClass(providerRef);
    } catch (error) {
      logger.error(`[PipelineEngine] åŠ è½½Providerå¤±è´¥ type=${type} path=${providerPath}`);
      throw new Error(`ProvideråŠ è½½å¤±è´¥: ${type}`);
    }
  }

  /**
   * å¤„ç†PipelineæˆåŠŸ
   * @param {string} taskId - ä»»åŠ¡ID
   * @param {Object} finalOutput - æœ€ç»ˆè¾“å‡ºç»“æœ
   */
  async handlePipelineSuccess(taskId, finalOutput) {
    try {
      const updateData = {
        status: 'success',
        artifacts: JSON.stringify(finalOutput),
        completed_at: new Date(),
        updated_at: new Date()
      };

      // å¦‚æœfinal outputåŒ…å«resultUrls,ä¿å­˜åˆ°æ—§å­—æ®µå…¼å®¹
      if (finalOutput.resultUrls) {
        updateData.resultUrls = JSON.stringify(finalOutput.resultUrls);
      }

      await db('tasks')
        .where('id', taskId)
        .update(updateData);

      logger.info(`[PipelineEngine] ä»»åŠ¡æˆåŠŸå®Œæˆ taskId=${taskId}`);

    } catch (error) {
      logger.error(`[PipelineEngine] æ›´æ–°ä»»åŠ¡æˆåŠŸçŠ¶æ€å¤±è´¥ taskId=${taskId}`, error);
    }
  }

  /**
   * å¤„ç†Pipelineå¤±è´¥
   * @param {string} taskId - ä»»åŠ¡ID
   * @param {string} featureId - åŠŸèƒ½ID
   * @param {number} failedStepIndex - å¤±è´¥çš„æ­¥éª¤ç´¢å¼•
   * @param {string} errorMessage - é”™è¯¯ä¿¡æ¯
   */
  async handlePipelineFailure(taskId, featureId, failedStepIndex, errorMessage) {
    try {
      // 1. æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºfailed
      await db('tasks')
        .where('id', taskId)
        .update({
          status: 'failed',
          error_message: errorMessage,
          errorReason: `æ­¥éª¤${failedStepIndex + 1}æ‰§è¡Œå¤±è´¥`,
          completed_at: new Date(),
          updated_at: new Date()
        });

      // 2. è·å–ä»»åŠ¡ä¿¡æ¯ç”¨äºè¿”è¿˜é…é¢
      const task = await db('tasks').where('id', taskId).first();
      if (!task) {
        logger.error(`[PipelineEngine] ä»»åŠ¡ä¸å­˜åœ¨ taskId=${taskId}`);
        return;
      }

      // 3. è·å–åŠŸèƒ½å®šä¹‰,è¿”è¿˜é…é¢
      const feature = await db('feature_definitions')
        .where('feature_id', featureId)
        .first();

      if (feature && task.userId) {
        // ğŸ”¥ ä¿®å¤å‚æ•°é¡ºåºï¼štaskIdåœ¨å‰ï¼ŒuserIdåœ¨å
        const result = await quotaService.refund(
          taskId,
          task.userId,
          feature.quota_cost,
          `Pipelineå¤±è´¥è¿”è¿˜:${taskId}`
        );

        if (result.refunded) {
          logger.info(
            `[PipelineEngine] é…é¢å·²è¿”è¿˜ taskId=${taskId} ` +
            `userId=${task.userId} amount=${feature.quota_cost}`
          );
        }
      }

      logger.error(
        `[PipelineEngine] Pipelineæ‰§è¡Œå¤±è´¥ taskId=${taskId} ` +
        `failedStep=${failedStepIndex} error=${errorMessage}`
      );

    } catch (error) {
      logger.error(`[PipelineEngine] å¤„ç†Pipelineå¤±è´¥å¼‚å¸¸ taskId=${taskId}`, error);
    }
  }

  /**
   * è¶…æ—¶Promiseè¾…åŠ©å‡½æ•°
   * @param {number} ms - è¶…æ—¶æ¯«ç§’æ•°
   * @param {string} message - è¶…æ—¶é”™è¯¯ä¿¡æ¯
   */
  timeout(ms, message) {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(message)), ms);
    });
  }

  /**
   * å»¶è¿Ÿå‡½æ•°
   * @param {number} ms - å»¶è¿Ÿæ¯«ç§’æ•°
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

module.exports = new PipelineEngine();
